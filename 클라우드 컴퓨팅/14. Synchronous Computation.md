# Synchronous Computation

(fully) synchronous application에서는 ***모든*** 프로세스들이 일정한 위치에서 동기화 된다.

`Barrier()` - generic function call

<img width="508" alt="스크린샷 2020-10-15 오후 3 55 25" src="https://user-images.githubusercontent.com/45806836/96087462-d8665800-0efe-11eb-8fc8-e3f9bf423368.png">


 - 프로세스들을 동기화 하기 위한 기본 매커니즘 : 모든 프로세스들이 기다려야 하는 곳에 Barrier( ) 호출을 삽입한다.
 - 모든 프로세스가 Barrier( ) 지점에 도착을 해야, 프로세스들은 그 지점을 떠날 수 있다.

- `MPI_Barrier(MPI_Comm comm);`

     - 커뮤니케이터 내의 모든 프로세스가 호출해야 한다.
     - 커뮤니케이터 안의 모든 프로세스들이 MPI_Barrier( ) 호출에 도달할 때까지 blocking 된다. (멈춘다)

## Barrier Implementation (tree)


<img width="870" alt="스크린샷 2020-10-15 오후 3 59 58" src="https://user-images.githubusercontent.com/45806836/96087842-7bb76d00-0eff-11eb-8682-6886bc60d8f2.png">


Barrier연산은 트리 구조로 이루어진다.

## Synchronous Computations

- Fully synchronous

    : 계산에 참여하는 ***모든*** 프로세스들이 동기화에 참여

- Locally synchronous

    : ***논리적으로 이웃*** 인 프로세스들끼리 동기화에 참여

### Examples

- 선형 연립 방정식의 해 구하기 (Solving a System of Linear Equations)
- 벽난로 문제 (Heat Distribution Problem)

---

## 선형 연립 방정식의 해 구하기 (Fully sync)

방정식 n 개와 미지수 n 개가 주어진다.

How? ***Jacobi Iteration***


<img width="789" alt="스크린샷 2020-10-15 오후 4 08 43" src="https://user-images.githubusercontent.com/45806836/96088648-b4a41180-0f00-11eb-94af-db2e196edbd4.png">


```c
x[i] = b[i]; /* initialize unknown */

for (iteration = 0; iteration < limit; iteration++) {
	sum = -a[i][i] * x[i]; /* 미리 하나 제외 */
	for (j = 0; j < n; j++) /* compute summation */
	sum = sum + a[i][j] * x[j];
	new_x[i] = (b[i] - sum) / a[i][i]; /* compute unknown */
	**allgather(&new_x[i]);** /* bcast/rec values */
	**global_barrier();** /* wait for all procs */
}
```

`MPI_Allgather()`

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1a201ff3-6e9d-406c-9fec-f2d8a8a47aea/_2020-10-15__4.08.33.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1a201ff3-6e9d-406c-9fec-f2d8a8a47aea/_2020-10-15__4.08.33.png)

어떤 프로세스가 가진 값을 모두 한 곳에 모은 후, broadcast

`Barrier()` 연산이 포함되어 있기 때문에 자동으로 동기화 → 추가로 쓸 필요x

## 벽난로 문제 (Locally sync)

방에 벽난로가 설치되어 있다. 방에 초기 온도와 벽난로의 온도가 주어졌을 때 한참 후의 방안 모든 곳의 온도를 계산해보자.

방을 2차원 배열로 mapping

time tk에서의 방 안의 온도 → tk+1 = 상하좌우 셀들의 평균값

time tk+1에서의 방 안의 온도는 h배열에 저장됨

병렬화: 

프로세스 Pi는 가장자리 셀들의 온도를 상하좌우 프로세스들에게 전달(Locally Sync)

가장 자리 셀들의 온도를 갱신하기 위해서 다른 프로세스가 가지고 있는 가장자리 셀의 온도를 추가로 저장해놓고 있어야 한다 → (m+2)*(m+2) 사이즈 필요


<img width="649" alt="스크린샷 2020-10-15 오후 4 34 35" src="https://user-images.githubusercontent.com/45806836/96091243-51b47980-0f04-11eb-9cf0-abe9e501e0f7.png">


*MPI 프로그램 특성상, 모든 프로세스에 큰 배열도 생성된다. 해결방법은?*

프로그램 실행 중에 배열들을 생성하면 된다. (Dynamic Allocate) 

프로세스가 담당할 영역을 다양하게 구성할 수 있다. 


<img width="507" alt="스크린샷 2020-10-15 오후 4 45 21" src="https://user-images.githubusercontent.com/45806836/96092429-d2c04080-0f05-11eb-81b0-6378af93022c.png">



- 각 프로세스가 담당하는 영역의 면적은 동일하다.
- 루프 실행 1회 후
    - 프로세스 사이의 통신 횟수, 데이터 전송량이 다르다.
    - 통신함수의 기본 오버헤드(걸리는 시간)와 데이터 전송 시간을 고려해 하나를 선택할 수 있다.
