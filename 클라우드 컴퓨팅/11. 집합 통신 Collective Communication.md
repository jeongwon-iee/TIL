# 집합 통신 Collective Communication

: Communicator 내의 모든 프로세스들이 참여하는 통신

 

## MPI: Collective Communication

모든 프로세스들이 참여, 파라미터에 TAG를 포함하지 않음

**MPI_Bcast( )** 

자신을 포함한 모든 프로세스들에게 데이터를 broadcast

- Broadcast from root to all other processes

**MPI_Gather( )** 

어떤 한 프로세스에 데이터를 순서대로 모아놓음

- Gather values for group of processes

**MPI_Scatter( )** 

어떤 프로세스가 갖고 있는 데이터를 모든 프로세스에게 같은 분량으로 나눠줌

- Scatters buffer in parts to group of processes

**MPI_Reduce( )** 

gather과 비슷. 차이는 데이터에 연산을 취하면서 모아옴

- Combine values on all processes to single value

**MPI_Alltoall( )** 

각 프로세스는 자신을 포함한 모든 프로세스에게 나눠줄 데이터를 갖고 있음

- Sends data from all processes to all processes

(함수 이름에 v가 붙으면 각 프로세스가 가진 데이터의 개수가 다를 수 있단 것)

## MPI_Bcast

어떤 프로세스가 가진 데이터를 모든 프로세스에게 보내주기

root: 보내주는 process 


Bcast는 모두에게 하는 것이니 if 문 밖에 써 줬음. 안에 써줘도 됨

모든 프로세스가 `MPI_Bcast`를 실행할 수 있는 시간이 되어야 단체로 실행

## MPI_Gather

모든 프로세스가 가진 데이터를 한 곳으로 모아오기


0번 (root)에게 퍼져있던 모든 데이터를 모아옴.

recv와 send buffer가 따로 필요


`1` ⇒ `각 프로세스`에서 오는 데이터 개수 (전체 개수 X)

## MPI_Allgather


특정 root에게만 X, 모든 프로세스들에게 모든 프로세스들의 데이터가 모임

gather 후 broadcast 하는 것과 결과는 동일하지만 연산 한 번으로 더 효율적.

## MPI_Reduce


모든 프로세스에게 연산을 수행하면서 한 프로세스에게 데이터를 모아줌

(덧셈을 하겠다고 지정한 상황)


배열을 대상으로 reduce 연산을 수행할 수 있다 (index가 같은 것끼리)

배열 하나를 여러 개의 프로세스로 분할하여 각각 덧셈 수행

## MPI_Allreduce


reduce 후 broadcast 한 연산과 결과가 같으나 연산 한 번이라 더 효율적

## MPI_Scatter


root 프로세스가 가진 값을 같은 분량으로 모든 프로세스가 나누어 가짐

## MPI_Alltoall


각 프로세스가 가진 데이터를 다같이 모든 프로세스에게 나눠줌

## Example


프로세스 0이 가진 전체 배열을 모두에게 broadcast 한 후 각자 맡은 부분들만 계산하여 총 합을 구하는 프로그램

→ 효율적이진 않음, 애초에 맡은 부분 배열만 `scatter` 해주는 게 더 효율적!
