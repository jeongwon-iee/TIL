# Derived Data Types

통신에 활용하기 위해 데이터 타입을 추가로 만듦

데이터 타입을 만드는 이유는 정수/실수/문자배열 이외의 다양한 데이터를 grouping하여 데이터 전송에 쓰기 위한 것.

동일한 분량의 데이터를 프로세스 사이에 전송할 때, 소량으로 나누어 여러 번에 걸쳐 전송하는 것보다 가능한 모아서 한꺼번에 전송하는 것이 효율적이다.
→ 통신 오버헤드 때문. 가능하면 통신 횟수를 줄이는 것이 좋다.


### 통신을 위한 데이터 그룹핑 (총 7가지 방법)

---

## 1. 배열의 일부 전송 : Count

배열의 일부를 전송하고자 할 때 전송하고자 하는 부분의 **시작 주소**와 전송할 배열의 **원소의 개수**를 통신 함수의 인수로 정해준다.


source가 destination으로 vector+2부터 3개의 데이터를 보내려 함.

데이터를 전송할 때 배열의 특정 부분을 전송할 때 이렇게 하면 된다.

⇒ 배열의 시작점과 개수 지정

## 2. Grouping Variables of Different Types

전송하려는 데이터의 특징

- 여러 타입의 데이터가 포함
- 각 타입마다 데이터 분량 상이
- Ex. 구조체 변수

⇒ 이러한 데이터 (변수)를 모두 모아서 하나의 타입으로 정의함

블록마다 타입/ 데이터 개수/ 시작 위치를 적어 줘야 함


정수형 배열, 실수형 배열, 문자 배열이 있고 이들을 한 데 모아 보내려고 함.

1. 배열마다의 타입을 적은 배열 선언 
(첫번째 block은 정수형이다, 두번째 block은 실수형이다, ...)
2. 배열마다의 길이를 적은 배열 선언
(첫번째 block은 원소가 5개 들어있다, 두 번째 block은 4개 들어있다, ...)
3. 배열마다 각 block의 위치를 적은 배열 선언
(정수 배열은 indata로부터 200만큼 떨어져 있다, ...)


`**MPI_Datatype` 타입 선언**

→ 타입 선언 시 적어 줘야 하는 두 줄

`MPI_Type_struct(3, block_lengths, displacements, typelist, &message_type);`

`MPI_Type_commit(&message_type);`

block이 세 개 있다, block 길이를 담은 배열, block 위치를 담은 배열, block 타입을 담은 배열, &message_type 에다 담음

이 message_type을 사용하겠다고 commit → 타입 완성!

이 메시지 타입은 모든 프로세스들이 참여해서 만들어줘야 함.

`MPI_Bcast(indata, count, message_type, root, MPI_COMM_WORLD);`

만든 데이터를 Bcast로 모든 프로세스에게 전송, 전송하려는 데이터의 시작점 indata 명시(데이터 타입을 만들 때는 기준점을 포함하지 않음), 몇 개를 보낼 것인가 count, 어떤 타입을 보낼 것인가 message_type, broadcast 하는 root, ...

구조체 배열을 보내려고 한다면 count가 1보다 클 것.

구조체 배열을 만들어 세번째 원소부터 보내고 싶다면 indata+3이라고 명시.

⇒ 구조체 변수나 구조체 배열도 메시지 하나에 담아 전송할 수 있다.

## 3.1 MPI_Type_contiguous

- 배열 안에 연속하여 저장되어 있는 원소들을 모아서 하나의 타입으로 정의
- 원소의 타입과 개수로 새로운 타입을 정한다.

⇒ 몇 개가 연속되는가? 원소 하나하나의 타입이 무엇인가?

```c
MPI_Datatype three; // three라는 새로운 datatype 선언

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

**MPI_Type_contiguous(3, MPI_FLOAT, &three);**
// three라는 타입을 만드려고 한다. 이 타입에는 실수 세 개가 연속된다.
**MPI_Type_commit(&three);** // 이 type을 사용하겠다!
```

type 선언은 collective하게 하는 것이기 때문에(모든 프로세스가 참여) if문장 안에서 하지 않음.

```c
if (myrank == 0) {
		for (int i=0;i<10;i++) vector[i]=i;
				MPI_Send(**vector+2, 1, three**, dest, tag, MPI_COMM_WORLD);
}
else if (myrank == 1) {
		MPI_Recv(**vector, 1, three**, source, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
		for (int i=0;i<10;i++) prinf("%.2f ", vector[i]);
		printf("\n");
}

MPI_Finalize();
```

### 배열의 일부를 전송하는 count와 contiguous의 차이?

예제 1)

```c
int i, myrank, A[20] = {0};
MPI_Datatype newtype;

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

if (myrank == 0)
		for (i=0;i<20;i++) A[i]=i;

MPI_Type_contiguous(3, MPI_INT, &newtype);
// 연속한 정수 3개로 된 데이터 타입 newtype 정의
MPI_Type_commit(&newtype); // newtype을 사용하겠다.

MPI_Bcast(A, 4, newtype, 0, MPI_COMM_WORLD); 
// newtype 데이터 4개를 전송한다. (배열 원소 총 12개가 전송됨)
// 데이터 시작점은 A 배열의 맨 앞

if (myrank == 2) {
		for(i=0;i<20;i++) printf("%d ", A[i]);
		printf("\n");
}
MPI_Finalize();
```


예제 2) 2차원 배열 상에서 배열의 1행에 대한 타입 정의


```c
MPI_Datatype rowtype;
int myrank, p, i, j, count = 4;

MPI_Init(NULL, NULL);
MPI_Comm_size(MPI_COMM_WORLD, &p);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

MPI_Type_contiguous(count, MPI_FLOAT, &rowtype);
// float 타입의 원소 4개를 rowtype이란 데이터타입으로 정의
MPI_Type_commit(&rowtype);
// rowtype을 사용하겠다

if (myrank == 0) {
		for(i=0;i<4;i++) {
				for(j=0;j<4;j++)
						a[i][j]=i*4+j;
		}
		
		for(i=0;i<4;i++) {
				for(j=0;j<4;j++)
						printf("%.1f ", a[i][j]);
				printf("\n");
		}

		MPI_Bcast(&a[1][0], 2, rowtype, 0, MPI_COMM_WORLD);
}
else 
		MPI_Bcast(&a[2][0], 2, rowtype, 0, MPI_COMM_WORLD);

if (myrank == 1) {
		for(i=0;i<4;i++) {
				for(j=0;j<4;j++)
						printf("%.1f ", a[i][j]);
				printf("\n");
		}
}

MPI_Finalize();
```



`MPI_Bcast()` 등 collective communication 함수는 모든 프로세스들이 함께 호출해야 된다. if 문 안에서 사용할 수도 있지만, 동기화 되어 실행되어야 하므로 프로세스들이 모두 실행할 수 있을 때까지 기다린 후 실행 된다.

## 3.2 MPI_Type_vector

배열에서 일정한 간격으로 저장되어 있는 원소들을 대상으로 타입을 정의



- 일정한 간격으로 떨어져 있는 원소들이 총 네 개 있음 (count)
- 각 block의 길이는 2 (block_length)
- 몇 칸을 주기로 나타나는가 세 칸 (stride)
- 하나에 대한 타입은 MPI_INT (element_type)

```c
int i, myrank, A[12] = {0};
MPI_Datatype newtype;

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

if (myrank == 0)
		for (i=0;i<12;i++) A[i]=i;

MPI_Type_vector(4, 2, 3, MPI_INT, &newtype);
// vector 데이터 타입 newtype 정의
// 총 4개의 block으로 구성되어 있으며, (count)
// 각 block은 2개의 원소로 이루어져 있고, (block_length)
// 각 block은 3개의 원소를 주기로 시작한다. (stride)
MPI_Type_commit(&newtype); // newtype을 사용하겠다.

MPI_Bcast(A, 1, newtype, 0, MPI_COMM_WORLD); 
// newtype 데이터 1개를 전송한다. 
// 데이터 시작점은 A 배열의 맨 앞

for(i=0;i<12;i++) printf("%d ", A[i]);
		printf("\n");
// 계산에 참여하는 모든 프로세스들이 출력

MPI_Finalize();
```


(참고) 여러 프로세스들이 직접 출력을 하는 경우에 출력 순서를 보장할 수 없다.

## 3.2 MPI_Type_vector : 배열의 열에 대한 타입 정의



2차원 배열은 실제로 저장될 때 1차원 배열처럼 저장이 됨, 1차원 배열 모양으로 펴서 본다면 배열의 열 개수를 주기로 한 열의 원소가 반복된다.

```c
MPI_Datatype columntype;
float A[4][4] = {0, 0};
int myrank, p, i, j, count = 4;

MPI_Init(NULL, NULL);
MPI_Comm_size(MPI_COMM_WORLD, &p);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

MPI_Type_vector(4, 1, 4, MPI_FLOAT, &columntype);
MPI_Type_commit(&columntype);
// rowtype을 사용하겠다

if (myrank == 0) 
		MPI_Bcast(&A[0][1], 1, columntype, 0, MPI_COMM_WORLD);
else 
		MPI_Bcast(&A[0][2], 1, columntype, 0, MPI_COMM_WORLD);

if (myrank == 1) {
		for(i=0;i<4;i++) {
				for(j=0;j<4;j++)
						printf("%.1f ", A[i][j]);
				printf("\n");
		}
}

MPI_Finalize();
```



## 3.2 MPI_Type_vector : 배열의 여러 열에 대한 타입 정의

```c
MPI_Type_vector(4, ***2***, 4, MPI_FLOAT, &columntype);
MPI_Type_commit(&columntype);
// rowtype을 사용하겠다

if (myrank == 0) 
		MPI_Bcast(&A[0][1], 1, columntype, 0, MPI_COMM_WORLD);
else 
		MPI_Bcast(&A[0][2], 1, columntype, 0, MPI_COMM_WORLD);
```



## 3.3 MPI_Type_Indexed

배열 안에 특정 규칙 없이 퍼져있는 원소들을 모아서 하나의 타입으로 선언할 때



배열의 검은 부분에 대해 하나의 타입으로 선언 하려 한다.

- count : block의 개수 (9)
- array_of_block_lengths : 각 block의 길이 (10, 9, 8, 7, 6, 5, 4, 3, 2, 1)
- array_of_displacements : 각 block의 시작 인덱스 (0, 11, 22, 33, 44, ... )

```c
int i, j, myrank;
**int count=10;
int array_of_block_lengths[10] = {10, 9, 8, 7, 6, 5, 4, 3, 2, 1};
int array_of_displacements[10] = {0, 11, 22, 33, 44, 55, 66, 77, 88, 99};**

float A[10][10]={0,0};
**MPI_Datatype uppertype;**

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

if (myrank == 0) {
		for (i=0 ;i<10; i++) {
				for (j=0; j<10; j++) {
						A[i][j]=i*4+j;
						printf("%5.1f", A[i][j]);
				}
		printf("\n");
		}
}

**MPI_Type_indexed(count, array_of_block_lengths, array_of_displacements, MPI_FLOAT, &uppertype);
MPI_Type_commit(&uppertype);**

MPI_Bcast(A, 1, uppertype, 0, MPI_COMM_WORLD);

if (myrank == 1) {
		for (i=0 ;i<10; i++) {
				for (j=0; j<10; j++) {
						A[i][j]=i*4+j;
						printf("%5.1f", A[i][j]);
				}
		printf("\n");
		}
}

MPI_Finalize();
```



## 3.4 MPI_Type_create_subarray

전체 배열에 대해서 작은 부분 배열의 데이터 타입을 만들 때

```c
int A[6][8]={0};
int array_of_sizes[ndims], array_of_subsizes[ndims], array_of_starts[ndims];
int i, j, myrank;

MPI_Datatype subarraytype;

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

if (myrank == 0) {
		for (i=0 ;i<6; i++) {
				for (j=0; j<8; j++) {
						A[i][j]=i*8+j;
						printf("%3d", A[i][j]);
				}
		printf("\n");
		}
}

array_of_sizes[0]=6; array_of_sizes[1]=8;
// 전체 배열 크기 6*8
array_of_subsizes[0]=3; array_of_subsizes[1]=5;
// 부분 배열 크기 3*5
array_of_starts[0]=2; array_of_starts[1]=1;
// 부분 배열 시작 인덱스 (2,1)

MPI_Type_create_subarray(ndims, array_of_sizes, array_of_subsizes
, array_of_starts, MPI_ORDER_C, MPI_INT, &subarraytype);
// 2차원 부분 배열 타입 정의, 배열 원소는 C언어 순서

MPI_Type_commit(&subarraytype);

MPI_Bcast(A, 1, subarraytype, 0, MPI_COMM_WORLD);
```



## MPI_Pacl, MPI_Unpack

데이터 타입을 정의하는 것은 아니고 데이터의 꾸러미처럼 Pack을 만들어 전송한 후, 받은 데에서 UnPack하여 데이터를 사용

```c
char buffer[100]; // 데이터들을 담을 꾸러미

MPI_Init(NULL, NULL);
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

if (myrank == 0) {
		scanf("%lf %lf %d, &a, &b, &n);
		p=0;
		MPI_Pack(&a, 1, MPI_DOUBLE, buffer, 100, &p, MPI_COMM_WORLD);
		MPI_Pack(&b, 1, MPI_DOUBLE, buffer, 100, &p, MPI_COMM_WORLD);
		MPI_Pack(&n, 1, MPI_INT, buffer, 100, &p, MPI_COMM_WORLD);
		MPI_Bcast(buffer, 100, MPI_PACKED, 0, MPI_COMM_WORLD);
}
else {
		MPI_Bcast(buffer, 100, MPI_PACKED, 0, MPI_COMM_WORLD);
		MPI_Unpack(buffer, 100, &p, &a, 1, MPI_DOUBLE, MPI_COMM_WORLD);
		MPI_Unpack(buffer, 100, &p, &b, 1, MPI_DOUBLE, MPI_COMM_WORLD);
		MPI_Unpack(buffer, 100, &p, &n, 1, MPI_INT, MPI_COMM_WORLD);
}

MPI_Finalize();
```
