# 병렬 하드웨어 (Parallel Hardware)

## 순차 컴퓨터 구조 (Serial Computer Architecture)

### 순차 (serial) 하드웨어와 소프트웨어

- Memory와 CPU가 1개씩 있고, Interconnect 로 연결되어 있다.
- Instruction (명령어)과 Data (처리할 데이터)는 메모리에 있다.
- CPU는 CU(control unit)와 ALU(arithmetic and logic unit)로 구성
- CU : 실행할 명령을 결정, ALU : 그 명령을 실행
- CPU 와 Memory 사이에 Interconnect 를 통해 명령과 데이터가 이동
- Interconnect 는 일반적으로 bus 구조
- 한 번에 **명령을 1개씩 처리**한다.
- **Memory 와 CPU 가 분리**되어 있어서 **von Neumann Bottleneck** 발생
- CPU의 계산 속도가 Memory와의 명령/데이터 이동 속도보다 현저히 큼.

      = CPU가 놀고있다. CPU의 활용도를 높여야 함.
    → 이를 해결하기 위한 (하드웨어적인) 노력은?

### 폰 노이만 모델의 성능 개선

***CPU 의 처리 속도를 높이는데 중점*** 을 둔다. (CPU 의 idle time 을 줄여보자.)

- CPU Cache (또는 Cache)
: CPU가 (메모리보다) 더 빠르게 엑세스 할 수 있는 ***캐시 메모리*** 를 둔다.
- 가상 메모리 (Virtual Memory)
- 대형 프로그램, 또는 대용량 데이터를 처리하는 경우
- 실행 중인 프로그램의 **일부만** 주기억장치 에 유지한다

        → 더 많은 프로그램 실행
    - 주기억장치를 보조기억장치의 캐시처럼 운영한다.


                                     성능 개선 →

### 폰 노이만 모델의 성능 개선 방법

- 명령어 레벨 병렬화 (ILP: instruction level parallelism)

: 여러 프로세서 또는 functional unit 들이 **동시에 명령어를 실행**

- 파이프라이닝 (pipelining)

   
- 다중 이슈 (multiple issue) : 여러 개의 명령어를 동시에 처리

    → 명령어들 사이에 dependency가 없어야 함

    
- 쓰레드 레벨 병렬화 (TLP: thread level parallelism)

: 현재 실행 중인 작업이 중단(stall)되었을 때 시스템이 다른 쓰레드를 실행시키면서 유용한 작업을 계속 할 수 있도록 한다.

- Coarse-grained Multithreading

: 쓰레드 1개가 실행되다가 **costly stall**(Ex. IO 작업) 되면 다른 쓰레드의 실행이 시작

- Fine-grained Multithreading

: 여러 쓰레드들이 **명령어 단위로** round-robin fashion 으로 번갈아 실행, Stall된 쓰레드는 건너뜀

- Simultaneous Multithreading (SMT)

: **같은 사이클에 여러 쓰레드의 명령들이 동시에 실행**, **다중 이슈**가 가능한 하드웨어 상에서 가능

---

## 병렬 하드웨어 (Parallel Hardware)

- 앞서 봤던 하드웨어 병렬화 (다중 이슈와 파이프라이닝)는 병렬 하드웨어 X

    → 프로그래머가 인지하지 못 하는 가운데 컴퓨터 속도가 빠를 뿐

- 병렬 하드웨어는 ***프로그래머가 소스코드를 수정하여 하드웨어의 병렬 특성을 이용할 수 있는 경우*** 로 국한

## 병렬 컴퓨터 분류 (Flynn's Taxanomy)

분류 기준: **명령어 스트림의 개수**와 **"동시 처리 가능한" 데이터 스트림의 개수**를 기준으로 병렬 하드웨어들을 분류


- SISD

매순간 명령어가 하나씩만 처리, 처리 되는 data도 하나 (**순차 컴퓨터**)

- MISD

여러 명령이 동시에 실행, 대상이 되는 data는 하나

- SIMD

- MIMD

---

## 병렬 하드웨어 분류 - SIMD

: Single Instruction Stream, Multiple Data Stream System 


- **명령 1개가 여러 데이터를 대상으로 동시에 처리**된다.
- 하나의 CU와 여러 대의 ALU로 구성되는 것으로 볼 수 있다.
- CU 에서 명령이 ALU 들로 broadcast 된다.

    = instruction 한 개를 모든 ALU에 보내준다.

- ALU는 자신에게 전달된 데이터에 그 명령을 실행 시키거나 idle 상태로 그냥 있을 수도 있다.
- 모든 ALU는 동일한 명령을 실행하거나 idle 상태임.
- 같은 시각에 서로 다른 명령을 실행하는 ALU 들은 없다.

→ 명령어 한 개를 모든 ALU들이 처리한다.

Ex. if (ALU 번호가 짝수이면) 자신이 받은 데이터에 2를 곱한다


ALU가 n개 있고, 명령어들이 순차적으로 control unit으로 들어옴.

- 명령어: ALU 번호가 i일 때 모든 x[i]값을 가져오시오.
- 명령어: ALU 번호가 i일 때 모든 y[i]값을 가져오시오.
- 명령어: 모든 ALU는 가진 x[i]값과 y[i]값을 더해 x[i]에 저장 하시오.

⇒ single instruction stream, multiple data stream

⇒ 여러 데이터가 동시에 처리 되더라도, 특정 시점에 처리되는 명렁어는 하나다.

### 단점

- 모든 ALU 들은 동일한 명령을 처리하거나 idle 상태에 있게 된다.
- 전통적인 모델에서는, 모든 ALU 가 동기화 (synchronized) 되어 작동하기도 하였다.
- ALU 에는 명령들을 저장할 곳이 없다.
- data parallel 특징을 가졌으며(= 같은 작업을 여러 데이터에 처리하는 모델) , 처리할 데이터가 방대한 경우에 적합한 모델이다.
- 복잡한 구조를 갖는 병렬 문제 처리에는 적합하지 않다.

### 유용한 분야들

- 벡터 프로세서 (Vector Processors): 배열이나 벡터를 대상으로 연산을 병렬적으로 빨리 해주는 컴퓨터
- GPUs: 영상을 컴퓨터 화면에 렌더링 해줄 때 큰 픽셀 배열로 변환 시 하나의 명령어로 병렬 처리 가능

---

## 병렬 하드웨어 분류 - MIMD

: Multiple Instruction Stream, Multiple Data Stream System

- 다중 데이터 스트림에 다중 명령어 스트림이 동작한다.

    → 명령어 스트림 하나가 데이터 스트림 하나를 처리

- 완전히 독립적인 프로세싱 유닛 (processing unit, core) 들로 구성된다.

    각 core 는 자신만의 CU와 ALU를 갖는다.

    → CPU 하나에 명령어 하나, 데이터 하나

- SIMD 시스템과는 달리 비동기적 (asynchronous) 이다.
    - Global clock 이 없다.
    - 각 core는 동일한 명령 시퀀스를 실행한다고 해도, (프로그래머가 강제로 동기화 하지 않는 한) 각자 실행 속도가 다를 수 있다.
- 종류 (CPU는 동일하게 여러 개, 메모리는?)
    - 공유 메모리 시스템 (Shared-memory System)

        ⇒ CPU들이 공유 메모리를 공유함

    - 분산 메모리 시스템 (Distributed-memory System)

        ⇒ CPU마다 독립적인 메모리 가짐

### MIMD: 공유 메모리 시스템 (Shared-Memory System)


- 자율적으로 동작하는 (autonomous) 프로세서들이 인터커넥션 네트워크를 통해 메모리 시스템에 연결되어 있다.
- 프로세서들은 메모리를 공유한다. 즉, 모든 프로세서들은 interconnect를 통해 각 메모리 셀에 접근할 수 있다.

    → CPU는 memory와 interconnect를 통해 연결, CPU끼린 연결 X

- 프로세서들은 대개 묵시적으로 (implicitly) 공유 데이터 구조에 엑세스함으로써 서로 통신한다.

    → 어떤 CPU가 Memory에 write한 data를 다른 CPU가 interconnect를 통해 read 할 수 있다.

- 가장 널리 사용되는 공유 메모리 시스템들은  하나 이상의 멀티 코어 프로세서들을 사용한다. (하나의 칩 상에 여러 CPU 또는 Core 가 있음)

    → **멀티 코어 시스템 (Multicore System)**

    : 침 하나에 여러 개의 core(CPU)들이 내장

    - UMA 멀티 코어 시스템

        
        모든 core에서 메모리 액세스 하는데 걸리는 시간이 같다.

    - NUMA 멀티 코어 시스템

        
        메모리에 직접 연결된 core의 메모리 액세스 속도가 더 빠르다.

### MIMD: 분산 메모리 시스템 (Distributed-Memory System)

- 클러스터 (분산 메모리 시스템의 일종)

여러 대의 PC들이 서로 Ethernet 같은 interconnect로 연결, 노드 하나엔 CPU와 Memory가 있으며 서로 독자적임. 하나의 시스템처럼 동작

→ 분산 메모리 시스템의 interconnect는 CPU와 Memory가 아닌 각 CPU-Memory 쌍들을 연결


---

## 인터커넥션 네트워크 (Interconnection Networks)

MIMD의 분산 메모리 시스템과 공유 메모리 시스템 모두에게 인터커넥션 네트워크는 시스템의 성능에 크게 영향을 끼친다.

- 공유 메모리 인터커넥트 (연결망)

     : CPU들과 Memory 사이 연결망

    Ex. Bus, Crossbars, ...

- 분산 메모리 인터커넥트 (연결망)

    : CPU와 Memory쌍들 간의 연결망

    - Direct interconnect

        Ex. Ring, Mesh, Torus, Hypercube, ...

    - Indirect interconnect

        Ex. Butterfly, Omega Network, ...

---

## 직접 연결망 (Direct interconnects)

### 인터커넥트: 공유 메모리 연결망

**버스 (Bus interconnect)**


- 병렬 통신을 위한 와이어 (wires) & 버스 엑세스를 제어하는 하드웨어로 구성된다.
- 버스에 연결된 디바이스들이 통신 와이어를 공유한다.
- 트래픽이 클 때, 대기 시간이 길어진다.

    ⇒ 특정 CPU가 Bus를 통해 Memory에 접근 중일 때 다른 CPU는 Memory에 접근할 수 없다.

- CPU와 메모리 사이의 통신에 사용된다.
- 장점: 값이 싸다.
- 단점: 버스에 연결된 디바이스의 개수가 많으면, 버스 사용 경쟁이 커서 성능이 저하된다.
- 버스에 연결할 수 있는 디바이스의 개수는 보통 8~16 정도이다.

**크로스바 (Crossbar)**
⇒ 모든 Processer(CPU)는 모두 Memory에 격자모양으로 연결 되어 있다.


연결지점이 스위치.

- 스위치 형태의 인터커넥션
- 디바이스 사이의 데이터 라우팅에 스위치가 사용된다.
- 서로 다른 디바이스 사이에는 동시 통신을 허락한다.

    ⇒ 병렬 access를 위해 메모리도 나누어져 있다.

- 버스보다 빠르다.
- 스위치와 링크에 드는 비용이 버스보다 비싸다.

### 인터커넥트: 분산 메모리 연결망

프로세서마다 별도의 메모리가 있는 구조 → 분산 메모리 시스템

- 프로세서들 사이의 통신을 위한 연결망이 있다.
- 연결망 분류
    - 직접 연결 방식 (Direct interconnect)

        
         - 각 스위치는 “프로세서와 메모리 쌍”에 직접 연결된다.
         - 스위치들끼리도 서로 연결되어 있다.

        Ex. Ring, Toroidal mesh(가로, 세로 방향으로 Ring)

        
    - 간접 연결 방식 (Indirect interconnect)

        
         - 스위치들이 프로세서에 직접 연결되지 않을 수도 있다.

### Bisection Width: 좋은 연결방향인지 판단하는 척도

: 동시에 가능한 통신 개수. 값이 클수록 많은 동시 통신 가능.

병렬 시스템을 (노드의 개수가 같은) 두 파티션으로 분할(**가장 나쁘게 분할**)하였다고 하자. 얼마나 많은 동시 통신이 두 파티션 사이에서 이루어질 수 있을까?


Ring 구조는 Bisection Width가 언제나 2


Bisection Width = 1


n*n Toroidal mesh는 Bisection Width가 2

### 완전 연결 네트워크 (Fully Connected Network)

: 각 스위치는 다른 모든 스위치와 연결 되어 있다.

이론적으론 최선의 네트워크이나 노드의 수가 많아지면 구축이 불가능하다.


### 하이퍼큐브 (Hypercube)

: 연결성이 매우 좋은 구조이다.


노드 개수가 2의 n승 (n이 차원 수를 결정)일 때, 하이퍼큐브의 Bisection Width는 2의 (n-1)승

어느 곳을 잘라도 Bisection width가 동일. 

d차원 하이퍼큐브의 경우 노드 하나에 링크가 d개이므로 한계가 있다.

---

## 간접 연결망 (Indirect interconnects)

: 스위치들이 모두 processor와 직접 연결되어 있지 않고, 스위치끼리의 네트워크를 이룸

- Crossbar
- Omega network

### Crossbar Interconnect

: 격자모양으로 스위치를 구성

- 프로세서 2개가 동일한 프로세서로 통신을 시도하지 않는 한, 모든 프로세서는 다른 프로세서와 동시에 통신을 할 수 있다.


### 오메가 네트워크 (omega network)

Crossbar에서 스위치를 조금 줄인 것. 좀 더 현실성 있음

